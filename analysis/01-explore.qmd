---
title: "Modelling eel abundance in the Vilaine bassin"
format: html
execute:
    message: false
    warning: false
---

# 0. Setup

Load dependencies.

```{r}
#| output: false
library(renv)
renv::load()
renv::restore() # Install missing librairies.
renv::status() # Check the project state.
```

```{r}
library(zen4R)
library(here)
```

Download data from Zenodo.
The data file is relatively heavy (~800Mb), this operation can take a few minutes.

```{r}
doi <- "10.5281/zenodo.17962542"
dest_dir <- here("data")
```

```{r}
#| eval: false
if (!dir.exists(dest_dir)) dir.create(dest_dir, recursive = TRUE)
download_zenodo(doi, path = dest_dir)
unzip(here(dest_dir, "miste_data.zip"), exdir = dest_dir)
```

If everything went well, the data should have been downloaded in `data/zenodo/`.

# 1. River data 

First, we focus on the river geographic data.
Let's load it.

```{r}
base::load(here(dest_dir, "zenodo", "processed_data", "reseaux_hydrographiques.rda"))
```

For now, we will focus on the Vilaine bassin.
We can view it using `sfnetworks`.

```{r}
library(sf)
library(sfnetworks)
net <- as_sfnetwork(rht_vilaine)
plot(net, cex = 0.5)
```

:::{.callout-caution}
We can see that the hydrographic network is not entirely connected.
For example, a small portion in the south west is disconnected from the rest of the bassin.
We should keep this in mind for later, as it could bias our statistical model.
:::

We want to check that the network data is valid.
In particular, we want to verify that there is no duplicate in the nodes, or edges data.

```{r}
nodes <- net |>
    activate("nodes") |>
    st_as_sf()
sum(duplicated(nodes))
```


```{r}
library(dplyr)
library(tidyr)

edges <- net |>
    activate("edges") |>
    st_as_sf()
sum(duplicated(select(edges, c("from", "to"))))
```

We see that there is no duplication for the Vilaine network.
We can proceed to the next step.

# 2. Environmental data 

## 2.1 Load data

```{r}
base::load(here(dest_dir, "zenodo", "processed_data", "poissons_env_temp.RData"))

env
```

Let's rename column for clarity.

```{r}
env <- env |> rename(year = annee)
```

Station geographical coordinates are stored in `points_geo_<river>`.
We visualize them over the hydrographic network.

```{r}
plot(net, col = "black", cex = 0.5)
plot(points_geo_vilaine, col = "red", add = TRUE, pch = 16, cex = 0.5)
```

## 2.2. Visualisation of altitude data

To begin with, let's focus on a single environment variable: the altitude.

```{r}
d_env <- env |>
    select(c("sta_id", "pop_id", "altitude")) |>
    distinct()
d_env |> filter(is.na(altitude))
```

We see that we have missing values for two stations: 8853 and 13994.
Next, we want to relate altitude values to geographical coordinates.

```{r}
pts_vilaine <- points_geo_vilaine |>
    select(c("pop_id", "geometry")) |>
    left_join(d_env, by = "pop_id") |>
    filter(!is.na(altitude))
pts_vilaine
```

We see that the altitude is not measured on every sampling points, but only in 37 locations.
Let's plot them.

```{r}
library(ggplot2)
set_theme(theme_minimal())
options(
    ggplot2.continuous.colour = "viridis",
    ggplot2.continuous.fill = "viridis"
)

ggplot() +
    geom_sf(data = points_geo_vilaine, color = "grey80", alpha = 0.5) +
    geom_sf(data = pts_vilaine, aes(color = altitude), size = 5) +
    labs(
        color = "Altitude (m)"
    )
```

We see in grey every location were we have sample of fish abundances, and in colour where we have altitude data.
We have to interpolate altitude data for missing locations.
The simplest way to do it is to the "nearest neighbour" interpolation.

## 2.3. Interpolation of altitude data

We first build proximity polygons with the `terra::voronoi` function.

```{r}
library(terra)

d_elev <- select(pts_vilaine, c("geometry", "altitude"))
d_elev.vect <- terra::vect(d_elev)
v <- voronoi(d_elev.vect)
plot(v, "altitude")
points(d_elev.vect)
```

```{r}
v.sf <- st_as_sf(v)
pred <- st_intersection(v.sf, points_geo_vilaine)
d_elev.nn <- select(pred, c("altitude", "pop_id", "geometry"))

ggplot() +
    geom_sf(data = pred, aes(color = altitude)) +
    geom_sf(data = pts_vilaine, aes(color = altitude), size = 5) +
    labs(
        color = "Altitude (m)"
    )
```

Predicted altitudes seem consistent with the nearest neighbour interpolation.

## 2.4. Cross-validation of altitude interpolation

Now, that we have a prediction pipeline working we will estimate its performance using cross-validation.
To do so, we will do a LOOCV (Leave-One-Out Cross-Validation).

```{r}
n <- nrow(d_elev)
preds <- numeric(n) # Vector filled with 0s.
for (i in 1:n) {
    train <- d_elev[-i, ]
    test <- d_elev[i, ]
    v_train <- voronoi(terra::vect(train))
    preds[i] <- st_intersection(st_as_sf(v_train), test)$altitude
}
d_elev$altitude.loo <- preds

ggplot(d_elev, aes(x = altitude, y = altitude.loo)) +
    geom_point() +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
    labs(
        x = "Observed altitude (m)",
        y = "Predicted altitude (m)"
    )

rmse <- sqrt(mean((d_elev$altitude - d_elev$altitude.loo)^2))
rmse
```

# 3. Abundance data

## 3.1. Load data

```{r}
catches <- catches |>
    rename(
        species = esp_code_alternatif,
        year = annee,
        abundance = effectif
    )
catches

species_names <- unique(catches$species)
```

Next, let's add zeros where species are not found.

```{r}
catches <- catches |>
    pivot_wider(
        names_from = species,
        values_from = abundance,
        values_fill = list(abundance = 0)
    ) |>
    pivot_longer(
        cols = species_names,
        names_to = "species",
        values_to = "abundance"
    )

catches
```

## 3.2. Preliminary plots

Let's plot abundance histogram for each species.
As most species abundance are dominated by zeros, because of absence data,
we plot log(abundance + 1) instead of raw abundances.
This also implies that we will need to use **zero-inflated** distributions in
our statistical model to account for this absence data.

```{r}
catches |>
    ggplot(aes(x = abundance + 1)) + # add 1 to avoid log(0)
    geom_histogram(binwidth = 0.5) +
    scale_x_log10() +
    facet_wrap(~species)
```

Let's plot abundance time series for the 10 most abundant species.

```{r}
n_species <- 10
common_species <- catches |>
    group_by(species) |>
    summarise(mean_abundance = mean(abundance)) |>
    arrange(desc(mean_abundance)) |>
    slice(1:n_species) |>
    pull(species)

catches |>
    filter(species %in% common_species) |>
    group_by(year, species) |>
    summarise(mean_abundance = mean(abundance)) |>
    ggplot(aes(x = year, y = mean_abundance, colour = species)) +
    geom_line()
```

There is no clear trend, but we can't say much at this point because
trend may be masked by sampling efforts and environmental covariates.

::: {.callout-note}
Here it would be interesting to plot the number of operations per year to see the evolution of the sampling effort.
:::

```{r}
d_eel <- catches |> filter(species == "ANG")
d_eel.avg <- d_eel |>
    group_by(pop_id) |>
    summarise(abundance_mean = mean(abundance))
data <- inner_join(d_elev.nn, d_eel.avg, by = "pop_id")

ggplot(data, aes(x = altitude, y = abundance_mean)) +
    geom_point() +
    labs(
        x = "Altitude (m)",
        y = "Eel abundance"
    )
```

We clearly observe the expected trend of eel abundance with altitude.

Next, we want to model the effect of altitude, while accounting for time using `INLA`.

# 4. Statistical models

Data is ready to model eel abundances in space, time and with altitude.
In this section, we will build increasingly complex models.

Before doing anything fancy, let's load `INLA`, and scale our predictor variable (altitude) as it is good practice.

```{r}
library(INLA)

data <- inner_join(d_eel, d_elev.nn, by = "pop_id")
data$altitude_s <- scale(data$altitude)
```

## 4.1. Abundance distribution and overdisperion

Here, we focus on the choice of the distribution of our target variable: eel abundance.
We will focus particularly on overdispersion, as we know that eel distribution is zero-inflated due to absence data.
For the following model, we add a random effect for stations and year.
We will discuss in further details these points in following sections.

### 4.1.1. Poisson model

We begin with a poisson GLM.

```{r}
m.poisson <- inla(
    abundance ~ 1 + altitude_s +
        f(year, model = "iid") +
        f(pop_id, model = "iid"),
    family = "poisson",
    data = data,
    control.compute = list(config = TRUE)
)
summary(m.poisson)
```

We can plot the distribution of the fixed effect (altitude).

```{r}
marginal.altitude <- m.poisson$marginals.fixed$altitude_s
ggplot(as.data.frame(marginal.altitude)) +
    geom_line(aes(x = x, y = y)) +
    labs(
        x = "Altitude effect size",
        y = "Probability"
    )
```

We can compute the highest posterior density (HDP) credible interval.

```{r}
inla.hpdmarginal(0.97, m.poisson$marginals.fixed$altitude_s)
```

We can also sample from the posterior distribution.

```{r}
m.poisson.samples <- inla.posterior.sample(100, m.poisson)
```

We represent the sampled effect size of altitude against the marginal distribution obtained before.

```{r}
fun <- function(...) {
    altitude_s
}
altitude.sample <- inla.posterior.sample.eval(fun, m.poisson.samples)
tab <- data.frame(altitude = altitude.sample[1, ])
ggplot(tab, aes(x = altitude)) +
    geom_histogram(aes(y = after_stat(density)), bins = 18) +
    geom_line(data = as.data.frame(marginal.altitude), aes(x = x, y = y)) +
    labs(
        x = "Altitude effect size",
        y = "Density"
    )
```

Next, we want to simulate data from the model.
This step is important as it allows us to see if our model makes sense or not.
In particular, we identify where our model fails and improve it.

```{r}
alt_vals <- seq(-1, 3, length.out = 100)
y.sample <- inla.posterior.sample.eval(
    fun = function(...) {
        mu <- exp(Intercept + altitude_s * alt_vals)
        abundance <- rpois(length(mu), mu)
    },
    samples = m.poisson.samples
)

library(bayestestR)
y.sum <- apply(y.sample, 1, function(x) {
    y.hdi <- hdi(x, ci = 0.89)
    c(mean = mean(x), hdi_low = y.hdi$CI_low, hdi_high = y.hdi$CI_high)
})
data.sim <- data.frame(
    altitude_s = alt_vals,
    t(y.sum)
)

ggplot(data.sim, aes(x = altitude_s, y = mean)) +
    # geom_point(data = data, aes(x = altitude_s, y = abundance), colour = "grey80") +
    geom_ribbon(aes(ymin = hdi_low, ymax = hdi_high), fill = "steelblue", alpha = 0.3) +
    geom_line(colour = "steelblue") +
    labs(
        x = "Scaled altitude",
        y = "Eel abundance"
    )
```

Next, we want to plot actual data against this prediction.
Because we have not included spatial and temporal random effects while sampling the posterior, we are making predictions for an average site and year.
Therefore, we want to average abundance data over site and year.

```{r}
data.avg <- data |>
    group_by(altitude_s) |>
    summarise(
        abundance_avg = mean(abundance),
        abundance_var = var(abundance)
    )
data.avg

ggplot(data.sim, aes(x = altitude_s, y = mean)) +
    geom_point(data = data.avg, aes(x = altitude_s, y = abundance_avg), colour = "grey80") +
    geom_ribbon(aes(ymin = hdi_low, ymax = hdi_high), fill = "steelblue", alpha = 0.3) +
    geom_line(colour = "steelblue") +
    labs(
        x = "Scaled altitude",
        y = "Eel abundance"
    )
```

We see that our model capture the clear decreasing trend of eel abundances with altitude.

Let's investigate overdispersion with a posterior predictive check.

```{r}
y_pred_avg <- apply(y.sample, 1, mean)
y_pred_var <- apply(y.sample, 1, var)
pred_df <- data.frame(
    abundance_avg = y_pred_avg,
    abundance_var = y_pred_var
)
pred_df$type <- "Posterior predictive"
pred_df$altitude_s <- alt_vals
data.avg$type <- "Observed"
plot_df <- rbind(pred_df, data.avg)

ggplot(plot_df, aes(x = abundance_avg, y = abundance_var, colour = type)) +
    geom_point() +
    geom_abline(slope = 1, intercept = 1) +
    labs(
        x = "Observed mean abundance",
        y = "Observed variance in abundace"
    )
```

We see clearly that observed data is overdispersed (variance >> mean) compared to the data generated by our model.
This suggest that the poisson distribution is not suited for this task.

### 4.1.2. Negative binomial model

To account for overdispersion, we will test the negative binomial distribution.
Basically, we will repeat the previous section but with a model where poisson distribution is replaced with a negative binomial one.

```{r}

```

### 4.1.3. Model comparisons and checking overdispersion

## 4.2. Modelling time

## 4.3. Modelling space

# TODO

- [ ] Check model overdispersion with negative binomial.
- [ ] Visualise simulated data v. true data (on a map maybe).
- [ ] Try different time random effects (iid, ar1, rw1) and compare models.
- [ ] Visualise the temporal trend through the AR1 effect.
- [ ] Try different space random effects (iid, besag, spde) and compare models.
- [ ] Sample prior distribution.
- [ ] Sample posterior distribution and plot against actual data.
